\section{Fragmentation Microbenchmarks}\label{sec:fsa-microbenchmarks}

We present several simple microbechmarks, each designed around a write/update
pattern for which it is difficult to ensure both fast writes in the moment and
future locality.  These microbenchmarks isolate and highlight the effects of
both intra-file fragmentation and inter-file fragmentation and show the
performance impact aging can have on read performance in the worst cases.

\paragraph{Intrafile Fragmentation.} When a file grows, there may not be room
to store the new blocks with the old blocks on disk, and a single file's data
may become scattered.  

Our benchmark creates 10 files by first creating each file of an initial size,
and then appending between 0 and 100 4KiB chunks of random data in a
round-robin fashion until each file is 400KiB.  In the first round the initial
size is 400KiB, so each entire file is written sequentially, one at a time. In
subsequent rounds, the initial size becomes smaller, so that the number of
round-robin chunks increases until in the last round the data is written
entirely with a round-robin of 4KiB chunks. After all the files are written,
the disk cache is flushed by remounting, and we wait for 90 seconds before
measuring read performance.  Some file systems appear to perform background
work immediately after mounting that introduced experimental noise; 90 seconds
ensures the file system has quiesced.

The aging process this microbenchmark emulates is multiple files growing in
length. The file system must allocate space for these files somewhere, but
eventually the file must either be moved or will fragment.

Given that the data set size is small and the test is designed to run in a
short time, an fsync is performed after each file is written in order to defeat
deferred allocation. Similar results are obtained if the test waits for 5
seconds between each append operation. If fewer fsyncs are performed or less
waiting time is used, then the performance differences are smaller, as the file
systems are able to delay allocation, rendering a more contiguous layout.

\input{fsa-microbenchmarks-intra-plots}

The performance of these file systems on an HDD and SSD are summarized in
\cref{fig:micro:intra}. On HDD, the layout scores generally correlate
($-0.93$) with the performance of the file systems.  On SSD, the file systems
all perform similarly (note the scale of the y-axis).  In some cases, such as
\xfs, \ext, and \zfs, there is a correlation, albeit at a small scale.  For
\btrfs, \ext, \xfs, and \ftwofs, the performance is hidden by read-ahead in the
OS, or in the case of \btrfs, also in the file system itself. If we disable
read-ahead, shown in \cref{subfig:rr:ssd_raoff}, the performance is more
clearly correlated ($-.67$) with layout score.  We do note that this
relationship on an SSD is still not precise; SSDs are sufficiently fast that
factors such as CPU time can also have a significant effect on performance.

\input{fsa-microbenchmarks-intralayout-plots}

Because of the small amount of data and number of files involved in this
microbenchmark, we can visualize the layout of the various file systems, shown
in \cref{fig:mb-intra-layout}. Each block of a file is represented by a
small vertical bar, and each bar is colored uniquely to one of the ten files.
Contiguous regions form a colored rectangle.  The visualization suggests, for
example, that \ext both tries to keep files and eventually larger file
fragments sequential, whereas \btrfs and \ftwofs interleave the round robin
chunks on the end of the sequential data. This interleaving can help explain
why \btrfs and \ftwofs perform the way they do: the interleaved sections must
be read through in full each time a file is requested, which by the end of the
test takes roughly 10 times as long. \ext and \xfs manage to keep the files in
larger extents, although the extents get smaller as the test progresses, and,
by the end of the benchmark, these file systems also have chunks of interleaved
data; this is why \ext and \xfs's  dynamic layout scores decline.  \zfs keeps
the files in multiple chunks through the test; in doing so it sacrifices some
performance in all states, but does not degrade.

Unfortunately, this sort of visualization doesn't work for \betrfs, because
this small amount of data fits entirely in a leaf.  Thus, \betrfs will read all
this data into memory in one sequential read. This results is some read
amplification, but, on an HDD, only one seek.

\input{fsa-microbenchmarks-inter-plots}

\paragraph{Interfile Fragmentation.}\label{sec:interfile} Many workloads read
multiple files with some logical relationship, and frequently those files are
placed in the same directory. Interfile fragmentation occurs when files which
are related---in this case by being close together in the directory tree---are not
collocated in the LBA space.

We present a microbenchmark to measure the impact of namespace creation order
on interfile locality. It takes a given ``real-life'' file structure, in this
case the Tensorflow repository obtained from \texttt{github.com}, and replaces
each of the files by 4KiB of random data. This gives us a ``natural'' directory
structure, but isolates the effect of file ordering without the influence of
intrafile layout. The benchmark creates a sorted list of the files as well as
two random permutations of that list. On each round of the test, the benchmark
copies all of the files, creating directories as needed with {\tt cp
--parents}.  However, on the $n$th round, it swaps the order in which the first
$n\%$ of files appearing in the random permutations are copied. Thus, the first
round will be an in-order copy, and subsequent rounds will be copied in a
progressively more random order until the last round is a fully random-order
copy.

The results of this test are shown \cref{fig:micro:inter}.  On hard
drive, all the file systems except \betrfs and \xfs show a precipitous
performance decline even if only a small percentage of the files are copied out
of order. \ftwofs's performance is poor enough to be out of scale for this
figure, but it ends up taking over 4000 seconds per GiB at round 100; this is
not entirely unexpected as it is not designed to be used on hard drive. \xfs is
somewhat more stable, although it is 13-35 times slower than drive bandwidth
throughout the test, even on an in-order copy.  \betrfs consistently performs
around 1/3 of bandwidth, which by the end of the test is 10 times faster than
\xfs, and 25 times faster than the other file systems. The dynamic layout
scores are moderately correlated with this performance ($-0.57$).

On SSD, half the file systems perform stably throughout the test with varying
degrees of performance. The other half have a very sharp slowdown between the
in-order state and the 10\% out-of-order state. These two modes are reflected
in their dynamic layout scores as well. While \ext and \zfs are stable, their
performance is worse than the best cases of several other file systems.
\betrfs is the only file system with stable fast performance; it is faster in
every round than any other file system even in their best case: the in-order
copy. In this cases the performance strongly correlates with the dynamic layout
score ($-0.83$).
