\section{Introduction}\label{Introduction}

Key-value stores form an integral part of system infrastructure.  Google's
LevelDB and Facebook's RocksDB are widely used, both within and outside of
their companies. Their importance has spurred research into several aspects of
key-value store design, such as increasing write throughput, reducing write
amplification, and increasing
concurrency~\cite{DBLP:conf/usenix/PapagiannisSGB16,cassandra,scylla,DBLP:conf/sosp/RajuKCA17,facebook-rocksdb2018,DBLP:conf/sigmod/DayanI18,DBLP:conf/sigmod/DayanI19,DBLP:conf/sigmod/DayanAI17,DBLP:conf/spaa/BenderFFFKN07,DBLP:conf/soda/BrodalF03,DBLP:conf/pods/BenderBJKMPSSZ16:set,DBLP:conf/pods/BenderFJMMPX17,DBLP:journals/tos/LuPGAA17,DBLP:conf/usenix/ChanLLX18,DBLP:conf/icalp/ConwayFS18,DBLP:conf/soda/IaconoP12,DBLP:conf/usenix/WuXSJ15,DBLP:conf/usenix/MarmolSTR15,DBLP:conf/cloud/VasudevanKA12,DBLP:conf/fast/ShettySMASZ13,DBLP:conf/eurosys/Golan-GuetaBHK15,DBLP:journals/tos/ZuoHW19,DBLP:conf/fast/YaoWHZLXH19,DBLP:conf/fast/KourtisIK19,DBLP:conf/fast/KaiyrakhmetLNNC19,leveldb,bradleywhitepaper,DBLP:conf/usenix/BalmauDGZYAGK17}.

\sloppy
Existing key-value stores face new challenges with the increasing use of
high-performance NVMe solid state drives (SSDs) in industry. NVMe SSDs offer
substantially higher bandwidth (500K-600K IOPS) and lower latency (10-20
micro-seconds) than other SSDs.

\fussy
These key-value stores struggle to utilize all the available bandwidth in
modern SSDs.  For example, we find that for the common case of small key-value
pairs, RocksDB is able to use only 30\% of the bandwidth supplied by an
Optane-based Intel 905p NVMe SSD (even when using 20 or more cores).

We find that the bottleneck has shifted from the storage device to the CPU:
reading data multiple times during compaction, cache misses, and thread
contention cause RocksDB to be CPU-bound when running atop NVMe SSDs.  Thus,
there is a need to redesign key-value stores to avoid these CPU inefficiencies.
While KVell~\cite{DBLP:conf/sosp/LepersBGZ19}, a new research key-value store,
also tries to reduce CPU overhead, it presents a design optimized for large
key-value pairs. In particular, we show that KVell experiences an extreme
performance cliff when it does not have enough memory to hold its in-memory
index, a limitation also acknowledged by its authors.

\begin{figure}
      \centering
      \ref{highlights-legend} \\
      \begin{subfigure}{0.49\columnwidth}
         \begin{tikzpicture}
            \begin{axis}[
                  ybar=8pt,
                  bar width=40pt,
                  enlarge x limits=0.1,
                  width = \hsize,
                  height = \hsize, 
                  symbolic x coords = {Load, A, B, C, D, E, F},
                  xtick = data,
                  major x tick style = transparent,
                  ymin = 0,
                  ymax = 2950,
                  xticklabels = {}, %{Load:\\Write Amp, Load:\\Total I/O Amp, Run C:\\Read Amp},
                  yticklabel style = {rotate=90, font=\small},
                  ylabel near ticks,
                  /pgf/number format/1000 sep={},
                  nodes near coords,
                  %every node near coord/.append style={font=\small},
                  ylabel={Operationss/Second (Thousands)},
                  xlabel={Insertion Throughput},
                  label style={align=center},%, font=\small},
                  legend columns=5,
                  legend style={at={(0.98, 0.98)}, anchor=north east},
                  legend to name={highlights-legend},
                  skip coords between index={1}{7}
               ]
               \addplot[style={Plum,fill=Plum,mark=none} ]
               table [col sep=space] {data/ycsb-100b/splinterdb.csv};
               \addplot[style={RoyalBlue,fill=RoyalBlue,mark=none}]
               table [col sep=space] {data/ycsb-100b/rocksdb.csv};
               \addplot[style={ForestGreen,fill=ForestGreen,mark=none}]
               table [col sep=space] {data/ycsb-100b/pebblesdb.csv};
               \legend{\sysname, RocksDB, PebblesDB};
            \end{axis}
         \end{tikzpicture}
         %% \caption{Throughput on YCSB workloads. Load consists of 673M
         %% operations, E consists of 20M operations and all other workloads
         %% consist of 160M operations. Higher is better.}\label{fig:ycsb-100b}
      \end{subfigure}
      %\hspace{0.075\columnwidth}
      \begin{subfigure}{0.49\columnwidth}
         \begin{tikzpicture}
            \begin{axis}[
                  ybar=8pt,
                  bar width=40pt,
                  enlarge x limits=0.1,
                  width = \hsize,
                  height = \hsize, 
                  symbolic x coords = {load-write},
                  %xticklabel style = {align=center},
                  %xticklabels = {Write Amp, Total I/O Amp},
                  xticklabels = {},
                  xtick = data,
                  yticklabel style = {rotate=90},%, font=\small},
                  major x tick style = transparent,
                  ymin = 0,
                  ymax=11.5,
                  ylabel={Write Amplification},
                  ylabel near ticks,
                  %label style = {font=\small},
                  %xlabel={YCSB Write Amplification (24b keys, 100b values)},
                  xlabel={Write Amplification},
                  nodes near coords,
                  %every node near coord/.append style={font=\small},
                  nodes near coords style={/pgf/number format/.cd, fixed zerofill, precision=1},
                  %legend columns=1,
                  %legend style={font=\scriptsize, at={(0.02, 0.98)}, anchor=north west},
                  %skip coords between index={1}{3}
               ]
               \addplot[style={Plum,fill=Plum,mark=none}]
               table [col sep=space] {data/intro-amp/splinterdb.csv};
               \addplot[style={RoyalBlue,fill=RoyalBlue,mark=none}]
               table [col sep=space] {data/intro-amp/rocksdb.csv};
               \addplot[style={ForestGreen,fill=ForestGreen,mark=none}]
               table [col sep=space] {data/intro-amp/pebblesdb.csv};
               %\legend{\sysname, \sysname (12 threads), RocksDB, PebblesDB};
            \end{axis}
         \end{tikzpicture}
         %% \caption{I/O amplification on YCSB load and Run C workloads, as
         %% measured with iostat. Lower is better.}\label{fig:ycsb-amp-100b}
      \end{subfigure}
      \caption{YCSB load throughput and write amplification benchmark results with 24-byte keys and 100-byte values.}
      \label{fig:eval-highlights}
\end{figure}

We present \sysname, a key-value store designed for high performance on NVMe
SSDs. For example, on small key-value pairs, \sysname is able to fully utilize
the device bandwidth and achieves almost $2\times$ lower write amplification
than RocksDB (see \cref{fig:eval-highlights}).  We show that compared to
state-of-the-art key-value stores such as RocksDB and PebblesDB, \sysname is
able to ingest new data 6--28$\times$ faster (see \cref{fig:eval-highlights})
while using the same or less memory.  For queries, \sysname is 1.5-3$\times$
faster than RocksDB and PebblesDB.

Three novel ideas contribute to the high performance of \sysname: the
\emph{\datastruct}, a concurrent memtable that removes the insertion
scalability bottleneck, and a concurrent user-level cache that reduces
cache-interference in highly-concurrent settings.  All three components are
designed to enable the CPU to drive high IOPS without wasting cycles.

At the heart of \sysname is the \datastruct, a novel data structure that
combines ideas from log-structured merge trees and \bets.  The \datastruct
adapts the idea of size-tiering (also known as fragmentation) from key-value
stores such as Cassandra and PebblesDB and applies them to \bets to reduce
write amplification by reducing the number of times a data item is re-written
during compaction.  The \datastruct also enables localized, fine-grained
compactions that increase compaction concurrency across the entire tree.  By
enabling fine-grained, localized compactions, \datastructs push ideas from
PebblesDB to their logical conclusion.

In key-value stores such as RocksDB, all inserted data is first stored in an
in-memory component called the memtable. We find that the memtable in RocksDB
provides low concurrency and becomes the bottleneck when used on top of the
highly-concurrent \datastruct on NVMe devices. We redesigned the memtable for
high concurrency: similar to the \datastruct, the \sysname memtable is based on
a B-tree designed using a 4KB disk block size and the L3 cache-line size; as a
result, when data needs to be moved between the memtable and \datastruct, it
can be done using simple pointer manipulations, reducing the CPU cost.

Finally, key-value stores such as RocksDB and PebblesDB use the Linux page
cache, but we found the page cache ill-suited for high concurrency.  We
designed a new user-level concurrent cache for \sysname that uses fine-grained,
distributed reader-writer locks to avoid contention and ping-ponging of cache
lines, as well as a direct map to enable lock-free cache operations. All the
data read and written by \sysname flows through this concurrent cache.

%At the heart of \sysname is the \emph{\datastruct}, a novel data structure that
%combines ideas from log-structured merge trees and \bets. \datastruct
%adapts the idea of size-tiering (also known as fragmentation) from
%key-value stores such as Cassandra and PebblesDB and applies them to
%\bets. Size-tiering reduces write amplification by reducing the number
%of times a data item is re-written during compaction. In contrast to
%size-tiering in log-structured merge trees, size-tiering in
%\datastruct allows for localized, fine-grained compactions: different
%parts of the tree can be compacted at the same time, and a node can be
%partially compacted (corresponding to the data going to a child
%node). \datastruct compactions reduce both the IO and CPU cost, as the
%data is both read and written fewer times compared to log-structured
%merge trees.
%
%Another novel feature of the \datastruct is that it decouples
%traditional compaction into two parts: flushing data from one
%level to another, and merging data items in the same level. This
%decoupling serves two main purposes. The first is that is makes most
%compactions fully asynchronous: within a single node multiple
%compactions can occur concurrently, while at the same time new
%data can be flushed into and out of the node. As a result, insertion
%performance scales far more linearly in \sysname than in other
%key-values stores (\sref{sec:scaling})
%
%Second, decoupling allows optimizations for insertion workloads that
%exhibit locality (such as sequential insertions), while reducing write
%amplification by allowing data to skip levels of the data structure
%and the resulting compactions. The use of the \datastruct enables high \sysname
%performance for local insertion workloads and graceful
%degradation as the randomness in the workload increases. \alex{fix these
%  numbers} For example, in a mixed random/sequential insertion
%workload, \sysname's single-threaded insertion performance gradually
%increases from 429K insertions per second to 907K insertions per
%second as the workload becomes more sequential.  RocksDB on other
%hand, has essentially constant insertion performance of about 150K
%inserts per second until the workload becomes 100\% sequential, when
%throughput jumps to about 200K inserts per second.
%
%Due to the low latency of NVMe devices, reducing write amplification
%alone is not enough for good performance; CPU cost, cache misses, and
%concurrency also have to be tackled. In key-value stores like RocksDB,
%inserted data is first stored in an in-memory component called the
%memtable. Since all data has to flow through the memtable, its concurrency
%becomes crucial. We find that the
%memtable in RocksDB provides low concurrency, and becomes the
%bottleneck when used on top of the highly-concurrent \datastruct on
%NVMe devices. We redesigned the memtable for high concurrency; for
%example, the \sysname cache uses a direct map instead of a hash
%table to enable lock-free cache operations. Similar to the
%\datastruct, the \sysname memtable is based on a B-tree designed using
%4KB disk block size and the L3 cache-line size; as a result, when data
%needs to be moved between the memtable and \datastruct, it can be done
%using simple pointer manipulations, reducing the CPU cost.
%
%Key-value stores such as RocksDB and PebblesDB use the Linux page
%cache; we found the page cache ill-suited for \sysname since we wanted
%fine-grained locks and high concurrency. We designed a new user-level concurrent
%cache for \sysname that is used instead of the page cache. \sysname
%uses distributed reader-writer locks to avoid contention and
%ping-ponging of cache lines. All the data read and written by \sysname
%flows through its concurrent cache.

\sysname is not without limitations. Like all key-value stores based
on size-tiering, \sysname sacrifices the performance of small range
queries, although less than one might expect.  For large range
queries, \sysname can use the full device bandwidth. Similarly,
size-tiering is known to temporarily increase space usage until
multiple versions of a single data item are compacted
together. Finally, \sysname is meant for scenarios where good
performance is required when memory is low; if memory is plentiful
and query performance is not as important,
then key-value stores such as KVell might be a better fit. Despite
these limitations, \sysname represents an interesting new point in the
design spectrum for key-value stores.

In summary, the contributions of \sysname are as follows:
\begin{itemize}
   \item We introduce the \datastruct, which reduces write amplification and
      enables fine-grained concurrency in compaction operations
      (\cref{sec:design,sec:flush-compact,sec:splitting}).
   \item We design and build a highly-concurrent memtable that is able to drive
      enough operations to the underlying \datastruct (\cref{sec:details}).
   \item We combine the \datastruct, memtable, and user-level cache in
      \sysname, a key-value store that can fully utilize NVMe SSD bandwidth.
      (\cref{sec:eval}).
\end{itemize}
