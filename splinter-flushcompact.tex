\section{Size-Tiering with Workload-Driven Compaction}\label{sec:flush-compact}

One of the goals of \sysname is to get both the benefits of size
tiering and the benefits of \bet's workload-driven compaction and
flushing.  Size-tiering reduces write amplification of all
workloads. Workload-driven compaction and flushing further reduces
write amplification when the workload is not uniformly random.
%% The challenge is that workload-driven flushing flushes from a parent
%% to only one child---the child with the most pending messages in the
%% parent.  How can we do this efficiently when the messages for the
%% child may be spread across multiple branches of the parent?
%% We describe below how \sysname solves this problem.
Our flushing and compaction algorithm is designed to preserve
worst-case performance guarantees of size-tiered LSM trees while
exceeding their performance on non-random workloads.  

The challenge is that a flushing and compaction algorithm must balance
two competing objectives.  First, we want to move data from one level
of the tree to the next in large batches.  This is the key reason
that LSMs and \bets are so much faster than B-trees for insertions.
On the other hand, we want to limit the number of locations that must
be searched during a query.  This means we cannot accumulate data
indefinitely before merging it into lower levels of the tree.

We begin by explaining the structure of \datastruct nodes.  This
structure will enable us to cleanly accomplish the above goals while
also enabling us to skip some compactions when the workload allows
it.

\textbf{Node structure.}  Each trunk node has a list of branches,
sorted from oldest to newest.  The trunk node also stores, for each
child, the index of the next branch to be flushed to that child.
Branches are flushed to a child in chronological order, so all older
branches have already been flushed to that child, and all newer
branches are yet to be flushed to that child.  We say that a branch is
\defn{live} for a child if it hasn't been flushed to that child.  We
say that a message in a branch is \defn{live} if the message's branch
is live for the message's target child.  Finally, each trunk node
stores, for each child, a rough estimate of the number of live
messages for that child across all the parent's branches.  This
estimate is made by scanning the top-level nodes of the branches and
estimating the amount of data in each subtree that falls entirely
within the pivots for a child.  Since branches are packed, these
estimates are quite accurate (typically to within less than 1\%).

Trunk nodes have a fixed number of branches that they can hold.  In
the current \sysname implementation, each trunk node can hold up to 84
branches.  However, trunk nodes begin flushing when they have $3F$
live branches, which is typically far less than 84 (e.g. $F$ is in the
range of 8 to 16).  This extra capacity is used to enable nodes to
absorb new incoming branches while compacting old branches.

Branches may be referenced by more than one trunk node.  Each trunk
node knows the range of keys that it covers so, for example, when a
parent and child both refer to the same branch, the parent might refer
to all the messages in the branch, while the child refers to only the
subset of messages in the branch for keys covered by the child.
Branches are immutable and refcounted, so sharing branches is safe.

This node structure and branch sharing means that we can ``flush'' a
live branch from a node to one of its children by simply adding a
pointer to the branch to the child. We can then mark the branch as
dead for that child in the parent.  If the branch becomes completely
dead in the parent (i.e. dead for all of its children), then the
parent can delete its references to the branch, decrementing the
branch's refcount.  Thus flushes are extremely cheap---just a few
pointer and refcount updates.

\textbf{Flush then Compact.}  \sysname avoids some intermediate
compactions by using a ``flush-then-compact'' approach.  Each flush
may trigger some compactions and some further, recursive flushes (see
below for a description of \sysname's flushing policy).  The idea of
flush-then-compact is to perform all the recursive flushes first.
Only once all the branches have been moved as far down the trunk as
possible do we begin performing compactions.  This will enable some
branches to skip intermediate compactions within the tree.

Once all the flushes from a node have completed, \sysname initiates
background compactions on all the nodes that recieved new branches.
Each background compaction compacts only the new branches in that node.
Thus no message gets compacted twice without being flushed from one
trunk node to another.

Compaction does not interfere with other concurrent tasks.  The trunk
node is not locked during the compaction---other threads may perform
queries or flush more branches to or from the trunk node.  During this
time, the compaction thread constructs a new branch that is the
compaction of all its input branches.  When the compaction thread
finishes, it briefly locks the trunk node to replace the old branch
pointers with a pointer to the new, compacted branch.

This mechanism also reduces contention at the root, since no
compactions are ever performed in the root.  Rather, whenever the root
fills, branches are moved to some of its children and compactions are
performed there, immediately making room for new items in the root.

Note that compactions skip over portions of the branches that are not
relevant to the compaction.  For example, when we flush branches from
the root to one of its children, the branches may contain keys outside
the range covered by that child.  When these branches get
compacted in the child, tthe compaction process won't
even look at those keys.  We can do this efficiently because branches
are B-trees that support iterators starting anywhere in the branch.

\textbf{Flushing policy.} 
The memtable has a maximum size of $m$ messages. 
Once it reaches size $m$, it is added as a
new branch to the root trunk node.

Flushes from a trunk node to one of its children are triggered by two
conditions: either the trunk node has more than $Fm$ live data, or one
of the trunk node's children has more than $3F$ live branches, where
$F$ is the fanout of the trunk.  These two conditions serve distinct
performance objectives.

The too-much-live-data trigger works with the compact-then-flush
algorithm to detect localized insertion workloads and move them
quickly down the tree without performing unnecessary intermediate
compactions.

For example, imagine a sequential insertion workload.  These inserts
first go to the memtable.  Once the memtable fills, it is added as a
branch to the root.  Once the root accumuates $F$ such branches, it
will go over the max-live-data threshold, causing a flush.  This flush
will move all the branches down the tree towards their target leaf.
This will immediately cause the child to exceed the max-live-data
threshold, so the branches will get flushed towards their target leaf
again.  This will repeat until the branches reach the leaf, at which
time \sysname will perform a compaction (and will probably split the
leaf).  The next batch of inserts will go to the new leaf created by
the split.  Thus each message will be involved in only a constant
number of compactions, giving $O(1)$ a write- and pass-complexity.

This method automatically adapts to varying degrees of
locality.  For example, if the workload is half inserts to a single
leaf, then every other time the root fills, we will perform a flush
from the root to the target leaf.  Or, if the workload consists of
random inserts of keys that all fall within a subtree $T$ of height,
say, $h/2$, then every time the root fills, \sysname will flush all
the branches to the root of the subtree without performing
intermediate compactions, skipping half the compactions that would
occur in a size-tiered LSM tree.

This policy does not weaken the worst-case insertion performance
guarantees of a size-tiered LSM: each message undergoes at most one
compaction per level of the tree, and the height of the tree is still
$\log_F N/m$.

The second flushing trigger is designed to bound the number of filters and
branches that must be examined during a query.  Whenever there are more than
$3hF$ branches on a search path (where $h$ is the height of the trunk), at
least one of the trunk nodes will violate the max-live-branches condition.
This will trigger a flush and compaction, which will reduce the number of
branches checked by queries along that path.
